{
  "5ed5561a-7493-4a16-b02e-c1213213e934.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Basics of parameter estimation ◦Maximum Likelihood Estimation (MLE) ◦Maximum A Posteriori Estimation (MAP) MLE ❖If the sample are truly random, then they are good approximations of true probabilities, having an underlying probability distribution. These forms are defined by probability functions or probability density functions. ❖Maximum likelihood estimation (MLE) is particular method to estimate the parameters of a probability distribution. ❖MLE attempts to find the parameter values that maximize the likelihood function, given the observations. The resulting estimate is called a maximum likelihood estimate, which is also abbreviated as MLE. MLE ❖Suppose we have a random sample X = {x1, ……, xn} taken from a probability distribution having the probability mass function or probability density function p(x/θ) where x denotes a value of the random variable and θ denotes the set of parameters that appear in the function. ❖In maximum likelihood estimation, we find the value of θ that makes the value of the likelihood function maximum. MLE ❖For computation convenience, we define the log likelihood function as the logarithm of the likelihood function: ❖A value of θ that maximizes L(θ) will also maximise l(θ) and vice-versa. ❖Hence, in maximum likelihood estimation, we find θ that maximizes the log likelihood function. MLE-Example MAP ❖ is the posteriori probability and P(B) is called the prior probability. ❖In MAP, we try to find the value of B, such a way to maximize the posterior probability"
  ]
}