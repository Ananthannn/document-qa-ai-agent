{
  "ca42186b-1856-4d51-96de-067eac5979f8.pdf": [
    "Lecture Note Dosgna cog jou ae percephen alge General Sluie lite of percep Low yn Lh ve b He | | Wy Np ” xpwo + > Any : cx The ante ame cha dp “i Cel is ped ls Ave aclvalon 4 Cth )) lohen d _ outed: ij Qs Ww, = WD) QF O% j Aeon wend aale 9: oF Oud tharwtold = | nates peubual werghls one. wot a LG a O. AND grt assume Anlial w fpssslrtd value 2b wok qwen ossume @ Mar Baselios College of Engineering And Technology Lecture Note ton ae i y2 b- 2. Gud WI, = 0.6 Thaeshotd = Aaok wip Aal yro- The Akinth Aabi of ANP gale u A BB Y oO (a) O O y O 0 Abs I “ bed rest da (no Beak aver) >h2XO040:6xO0=0 (for snalcal 8 a aud Orla a No oa ae En sage F and agi aliens SOme - Mapa: B20 B=! Tovge F = gi’ Ox 2 + 1K 06 ; 5 - 0 . Tovget = Onlput Mar Baselios College of Engineering And Technology Lecture Note da W Kap d : 21, Boo, Tonge = fi? (Al24+0x06 = ('2 ial [ Noni fs be re ie pad t 4 C- od 12405(0-01 Wp Yew = - Opnyead = &-7 Waneas= OC: OF o s(0-1)e i td 56 we Por 0:6) _ past the paotee in @ B-o T + = O a i Mi : O*Ko-7 + 0K0:6 = 9 di = - _— Tonget = ofp Lecture Note BUep2 : A-O B=! Tonget 2 O Ui =OKO.7D + [x Ob = O46 Sa d = 0 rs ofp Alp 3 A>{[ B=0 Target = 0 fiz (K0-7 + OK 0.6207 Heer =0 Tonget = /P Ap 4: A:| =| Tough = | ie Ix0.7 4(xo.62 (3 a 13 | aie’ } Foaget = YP SAA a bade lin og Aatasfed - Rho es model wv “pe %h Mar Baselios College of of Engineering And Technology"
  ],
  "82d0bc9f-335f-4d9f-b7d0-c46788edf1ca.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET MULTILAYERED FEED FORWARD NETWORKS Single Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Multi Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Multi Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Feed-forward neural networks allows signals to travel from input to output. There is no feedback (loops) such as the output of some layer does not influence that same layer. This type of organization is represented as bottom-up or top-down. MULTILAYERED FEED FORWARD NETWORKS BACK PROPAGATION ALGORITHM ❖Backpropagation is a powerful algorithm in deep learning, primarily used to train artificial neural networks, particularly feed-forward networks. ❖In the backward pass, the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. ❖Parameter considered is Mean Squared Error (MSE)=(Predicted Output−Actual Output)2 BACK PROPAGATION ALGORITHM ❖Gradient descent search determines a weight vector that minimizes E by starting with an arbitrary initial weight vector, then repeatedly modifying it in small steps. ❖ At each step, the weight vector is altered in the direction that produces the steepest descent along the error surface depicted in Figure BACK PROPAGATION ALGORITHM The negative sign is present because we want to move the weight vector in the direction that decreases E. BACK PROPAGATION ALGORITHM BACK PROPAGATION ALGORITHM Gradient Descent ❖New Value=Old Value-Step Size ❖Step Size=learning rate* Slope ❖Consider an example f(x)=x^2 ❖If our aim is to reduce f(x), we find the derivative of f(x) and then equate it to zero. Example ❖New Value=Old Value-Step Size ❖Step Size=learning rate* Slope ❖Consider an example f(x)=x^2 ❖If our aim is to reduce f(x), we find the derivative of f(x) and then equate it to zero. Example The target is 0.5 and learning rate is 1 and consider sigmoid activation Example The target is 0.5 and learning rate is 1 and consider sigmoid activation Example The target is 0.5 and learning rate is 1 and consider sigmoid activation"
  ],
  "496ac076-fdb9-4c2d-9da5-226b4b37d92b.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Basics of parameter estimation ◦Maximum Likelihood Estimation (MLE) ◦Maximum A Posteriori Estimation (MAP) Bayesian classifier and ML estimation ❖The Bayesian classifier is an algorithm for classifying multiclass datasets. ❖This is based on the Bayes’ theorem in probability theory. ❖The classifier is also known as “Naive Bayes Algorithm”. ❖Conditional Probability-The probability of the occurrence of an event A given that an event B has already occurred is called the conditional probability of A given B and is denoted by P(A/B). ❖Bayes’ theorem-Let A and B any two events in a random experiment. If P(A) ≠ 0, then Naive Bayes Algorithm- Assumption and Background idea: ❖All the features are independent and are unrelated to each other. ❖The data has class-conditional independence ❖Suppose we have a training data set consisting of N examples having n features. ❖In this case N=10 and n=4 Naive Bayes Algorithm- Assumption and Background idea: ❖Let the features be named as (F1,………,Fn). ❖A feature vector is of the form (f1, f2, ………….,fn). ❖Associated with each example, there is a certain class label. ❖Let the set of class labels be {c1, c2, ……………,cp}. (Here p=3) ❖Suppose we are given a test instance having the feature vector ❖We are required to determine the most appropriate class label that should be assigned to the test instance X ❖For this purpose we compute the following conditional probabilities Naive Bayes Algorithm- Assumption and Background idea: ❖choose the maximum among them. ❖Let the maximum probability be ❖Then, we choose as the most appropriate class label for the training instance having X as the feature vector. ❖The conditional probability is computed using Baye’s Theorem, ❖Due to class conditional independence Naive Bayes Algorithm- Assumption and Background idea: ❖Due to class conditional independence ❖Substituting in Naive Bayes Algorithm- Assumption and Background idea: ❖Due to class conditional independence ❖Substituting in Naive Bayes Algorithm- Assumption and Background idea: Naive Bayes Algorithm- Assumption and Background idea: Naive Bayes Algorithm Naive Bayes Algorithm Example For simplicity, each unit is classified as “Animal”, “Bird” or “Fish”. Use naive Bayes algorithm to classify a particular species if its features are (Slow, Rarely,No) Example Example Example Example Example Example Example Example Example"
  ],
  "2833fdf5-1ac8-452f-8528-91bd36842170.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Basics of parameter estimation ◦Maximum Likelihood Estimation (MLE) ◦Maximum A Posteriori Estimation (MAP) MLE ❖If the sample are truly random, then they are good approximations of true probabilities, having an underlying probability distribution. These forms are defined by probability functions or probability density functions. ❖Maximum likelihood estimation (MLE) is particular method to estimate the parameters of a probability distribution. ❖MLE attempts to find the parameter values that maximize the likelihood function, given the observations. The resulting estimate is called a maximum likelihood estimate, which is also abbreviated as MLE. MLE ❖Suppose we have a random sample X = {x1, ……, xn} taken from a probability distribution having the probability mass function or probability density function p(x/θ) where x denotes a value of the random variable and θ denotes the set of parameters that appear in the function. ❖In maximum likelihood estimation, we find the value of θ that makes the value of the likelihood function maximum. MLE ❖For computation convenience, we define the log likelihood function as the logarithm of the likelihood function: ❖A value of θ that maximizes L(θ) will also maximise l(θ) and vice-versa. ❖Hence, in maximum likelihood estimation, we find θ that maximizes the log likelihood function. MLE-Example MAP ❖ is the posteriori probability and P(B) is called the prior probability. ❖In MAP, we try to find the value of B, such a way to maximize the posterior probability"
  ],
  "7eefe247-0fb0-4cd4-8194-850a56d709af.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Syllabus-Module I ❑Machine learning paradigms-supervised, semi-supervised,unsupervised, reinforcement learning. ❑Introduction to artificial neurons, Perceptron – perceptron algorithm, Neural Network - Multilayer feed forward network, Activation functions (Sigmoid, ReLU, Tanh), Backpropagation algorithm. ❑Basics of parameter estimation - maximum likelihood estimation (MLE) and maximum a posteriori Estimation (MAP). Syllabus-Module II ❑Regression - Linear regression with one variable, Linear regression with multiple variables, Logistic regression, gradient descent algorithm and matrix method, basic idea of overfitting in regression. ❑Classification- Introduction to Bayes decision theory, discriminant functions and decision surfaces. Syllabus-Module III ❑Ensemble methods: boosting, bagging. Decision trees algorithm ID3, random forest, Bootstrapping. ❑SVM - Introduction, Maximum Margin Classification, soft margin SVM classifier, non-linear SVM, Kernels for learning non-linear functions - polynomial kernel, Radial Basis Function (RBF). Syllabus-Module IV ❑Clustering - Similarity measures, Hierarchical Agglomerative Clustering, K-means partitional clustering, Expectation maximization (EM) for soft clustering. ❑Dimensionality reduction: principal component analysis, Fischer’s discriminant analysis. Syllabus-Module V ❑Classification Performance measures Confusion matrix,Precision, Recall, Accuracy, F-Measure, Receiver Operating Characteristic Curve (ROC) and Area Under Curve (AUC) and their significance. ❑Cross Validation, Bias-Variance decomposition. ❑Case Study: Developing an effective machine learning model for a real world classification or a regression problem. Course Outcomes Level CO1 Explain the basics of machine learning and neural networks. Understan d CO2 Discuss regression and classification theory in machine learning. Understan d CO3 Apply mathematical methods in machine learning models. Apply CO4 Explain supervised and unsupervised learning algorithms and non-metric methods. Understan d CO5 Summarize ensemble methods, dimensionality reduction, evaluation, model selection. Apply Text Books 1. Bishop, C. M. ‘Pattern Recognition and Machine Learning’, Springer, New York,2e, 2016. 2. Ethem Alpaydin ‘Introduction to Machine Learning (Adaptive Computation and Machine learning) 3e, MIT Press, 2014. 3. Mitchell.T, ‘Machine Learning’ McGrawHill, 1983 Assessment Pattern Continuous Assessment : End Semester Examination 40 : 60 Assignments: Assignment 1-based on all tutorials Assignment 2- Simulation What is Machine learning Machine Learning is all about machines learning automatically without being explicitly programmed or learning without any direct human intervention. This process starts with feeding them good quality data and then training the machines by building various machine learning models using the data and different algorithms. Machine learning allows computers to learn and infer from data. What is Machine learning Machine Learning algorithm learns from experience E with respect to some type of task T and performance measure P For example, if a ML algorithm is used to play chess. Then the experience E is playing many games of chess, the task T is playing chess with many players, and the performance measure P is the probability that the algorithm will win in the game of chess. Supervised learning Types of machine learning ❑Supervised Learning ❑Unsupervised Learning ❑Semi Supervised Machine Learning- ◦This is a combination of Supervised and Unsupervised Machine Learning ◦Uses a little amount of labeled data like Supervised Machine Learning and a larger amount of unlabeled data like Unsupervised Machine Learning to train the algorithms. Types of machine",
    "learning Reinforced Machine Learning- ◦learn optimal actions through trial and error. ◦algorithm decides the next action by learning behaviors that are based on its current state and that will maximize the reward in the future ◦This is done using reward feedback Data in machine learning DATA: It can be any unprocessed fact, value, text, sound, or picture that is not being interpreted and analyzed. INFORMATION: Data that has been interpreted and manipulated and has now some meaningful inference for the users. KNOWLEDGE: Combination of inferred information, experiences, learning, and insights. How we split data in Machine Learning Training Data-: The part of data we use to train our model. This is the data that your model actually sees(both input and output) and learns from. Validation Data-The part of data that is used to do a frequent evaluation of the model, fit on the training dataset along with improving involved hyperparameters (initially set parameters before the model begins learning). Testing Data-The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
  ],
  "619ddea7-7177-4d10-978c-736ece9ddd3f.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET BIOLOGICAL NEURAL NETWORK BIOLOGICAL NEURAL NETWORK • 10^11 neurons in total • each connected to 10^4 neurons on average • Multiple signals arrive at the dendrites and are then integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated that will be passed on by the axon. Artificial Neural Network (ANN) The study of artificial neural networks (ANNs) has been inspired in part by the observation that biological learning systems are built of very complex webs of interconnected neurons. In rough analogy, artificial neural networks are built out of a densely interconnected set of simple units, where each unit takes a number of real-valued inputs (possibly the outputs of other units) and produces a single real-valued output (which may become the input to many other units). Artificial Neural Network (ANN) While ANNs are loosely motivated by biological neural systems, there are many complexities to biological neural systems that are not modelled by ANN. Many features of the ANNs are known to be inconsistent with biological systems. For example, in ANNs, individual units output a single constant value, whereas biological neurons output a complex time series of spikes. Artificial Neuron The artificial neuron has the following characteristics: – A neuron is a mathematical function modelled on the working of biological neurons – It is an elementary unit in an artificial neural network – One or more inputs are separately weighted – Inputs are summed and passed through a nonlinear function to produce output – Each connection link carries information about the input signal – Every neuron is connected to another neuron via connection link Perceptrons Perceptrons A perceptron takes a vector of real-valued inputs, calculates a linear combination of these inputs, then outputs a 1 if the result is greater than some threshold and -1 otherwise. A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features in the input data. Perceptron Perceptron was introduced by Frank Rosenblatt in 1957. A Perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurons to learn and processes elements in the training set one at a time. There are two types of Perceptrons: Single layer and Multilayer. ◦Single layer Perceptrons can learn only linearly separable patterns. ◦Multilayer Perceptrons or feedforward neural networks with two or more layers have the greater processing power. • ◦The Perceptron algorithm learns the weights for the input signals in order to draw a linear decision boundary. • This enables you to distinguish between the two linearly separable classes +1 and -1. Perceptrons Weight parameter represents the strength of the connection between units. Activation Function: These are the final and important components that help to determine whether the neuron will fire or not. Activation Function can be considered primarily as a step function. Sign function Step function Sigmoid function Activation Functions Perceptrons Perceptrons Perceptrons CS 270 - PERCEPTRON 15",
    "Perceptron Node – Threshold Logic Unit q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 z x1 xn x2 w1 w2 wn q𝜃 CS 270 - PERCEPTRON 16 Perceptron Learning Algorithm x1 x2 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 .4 -.2 .1 x1 x2 t 0 1 .1 .3 .4 .8 CS 270 - PERCEPTRON 17 First Training Instance 0.8 0.3 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 0.4 -0.2 .1 net = 0.8*0.4 + 0.3*-0.2 = 0.26 =1 x1 x2 t 0 1 0.1 0.3 0.4 0.8 CS 270 - PERCEPTRON 18 Second Training Instance .4 .1 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 .4 -.2 .1 x1 x2 t 0 1 .1 .3 .4 .8 net = .4*.4 + .1*-.2 = .14 =1 wi = (t - z)  c  xi Perceptron Training Rule precise learning problem is to determine a weight vector that causes the perceptron to produce the correct f 1 output for each of the given training examples. y to learn an acceptable weight vector is to begin with random weights, then iteratively apply the perceptron to each training example, modifying the perceptron weights whenever it misclassifies an example. This process is repeated, iterating through the training examples as many times as needed until the perceptron classifies all training examples correctly. The role of the learning rate is to moderate the degree to which weights are changed at each step. It is usually set to some small value (e.g., 0.1) CS 270 - PERCEPTRON 20 Perceptron Rule Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: wi = c (t – z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) W 0 0 1 1 0 0 0 0 0 CS 270 - PERCEPTRON 21 Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: wi = c(t – z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) W 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0",
    "CS 270 - PERCEPTRON 22 Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: wi = c (t – z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) W 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1……………………………………………… Perceptrons-Hyperplanes We can view the perceptron as representing a hyperplane decision surface in the n- dimensional space of instances (i.e., points). The perceptron outputs a 1 for instances lying on one side of the hyperplane and outputs a -1 for instances lying on the other side Perceptrons Perceptrons"
  ]
}