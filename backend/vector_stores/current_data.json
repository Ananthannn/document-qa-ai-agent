{
  "ca42186b-1856-4d51-96de-067eac5979f8.pdf": [
    "Lecture Note Dosgna cog jou ae percephen alge General Sluie lite of percep Low yn Lh ve b He | | Wy Np ‚Äù xpwo + > Any : cx The ante ame cha dp ‚Äúi Cel is ped ls Ave aclvalon 4 Cth )) lohen d _ outed: ij Qs Ww, = WD) QF O% j Aeon wend aale 9: oF Oud tharwtold = | nates peubual werghls one. wot a LG a O. AND grt assume Anlial w fpssslrtd value 2b wok qwen ossume @ Mar Baselios College of Engineering And Technology Lecture Note ton ae i y2 b- 2. Gud WI, = 0.6 Thaeshotd = Aaok wip Aal yro- The Akinth Aabi of ANP gale u A BB Y oO (a) O O y O 0 Abs I ‚Äú bed rest da (no Beak aver) >h2XO040:6xO0=0 (for snalcal 8 a aud Orla a No oa ae En sage F and agi aliens SOme - Mapa: B20 B=! Tovge F = gi‚Äô Ox 2 + 1K 06 ; 5 - 0 . Tovget = Onlput Mar Baselios College of Engineering And Technology Lecture Note da W Kap d : 21, Boo, Tonge = fi? (Al24+0x06 = ('2 ial [ Noni fs be re ie pad t 4 C- od 12405(0-01 Wp Yew = - Opnyead = &-7 Waneas= OC: OF o s(0-1)e i td 56 we Por 0:6) _ past the paotee in @ B-o T + = O a i Mi : O*Ko-7 + 0K0:6 = 9 di = - _‚Äî Tonget = ofp Lecture Note BUep2 : A-O B=! Tonget 2 O Ui =OKO.7D + [x Ob = O46 Sa d = 0 rs ofp Alp 3 A>{[ B=0 Target = 0 fiz (K0-7 + OK 0.6207 Heer =0 Tonget = /P Ap 4: A:| =| Tough = | ie Ix0.7 4(xo.62 (3 a 13 | aie‚Äô } Foaget = YP SAA a bade lin og Aatasfed - Rho es model wv ‚Äúpe %h Mar Baselios College of of Engineering And Technology"
  ],
  "82d0bc9f-335f-4d9f-b7d0-c46788edf1ca.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET MULTILAYERED FEED FORWARD NETWORKS Single Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Multi Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Multi Layered Feedforward network MULTILAYERED FEED FORWARD NETWORKS Feed-forward neural networks allows signals to travel from input to output. There is no feedback (loops) such as the output of some layer does not influence that same layer. This type of organization is represented as bottom-up or top-down. MULTILAYERED FEED FORWARD NETWORKS BACK PROPAGATION ALGORITHM ‚ùñBackpropagation is a powerful algorithm in deep learning, primarily used to train artificial neural networks, particularly feed-forward networks. ‚ùñIn the backward pass, the error (the difference between the predicted and actual output) is propagated back through the network to adjust the weights and biases. ‚ùñParameter considered is Mean Squared Error (MSE)=(Predicted Output‚àíActual Output)2 BACK PROPAGATION ALGORITHM ‚ùñGradient descent search determines a weight vector that minimizes E by starting with an arbitrary initial weight vector, then repeatedly modifying it in small steps. ‚ùñ At each step, the weight vector is altered in the direction that produces the steepest descent along the error surface depicted in Figure BACK PROPAGATION ALGORITHM The negative sign is present because we want to move the weight vector in the direction that decreases E. BACK PROPAGATION ALGORITHM BACK PROPAGATION ALGORITHM Gradient Descent ‚ùñNew Value=Old Value-Step Size ‚ùñStep Size=learning rate* Slope ‚ùñConsider an example f(x)=x^2 ‚ùñIf our aim is to reduce f(x), we find the derivative of f(x) and then equate it to zero. Example ‚ùñNew Value=Old Value-Step Size ‚ùñStep Size=learning rate* Slope ‚ùñConsider an example f(x)=x^2 ‚ùñIf our aim is to reduce f(x), we find the derivative of f(x) and then equate it to zero. Example The target is 0.5 and learning rate is 1 and consider sigmoid activation Example The target is 0.5 and learning rate is 1 and consider sigmoid activation Example The target is 0.5 and learning rate is 1 and consider sigmoid activation"
  ],
  "496ac076-fdb9-4c2d-9da5-226b4b37d92b.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Basics of parameter estimation ‚ó¶Maximum Likelihood Estimation (MLE) ‚ó¶Maximum A Posteriori Estimation (MAP) Bayesian classifier and ML estimation ‚ùñThe Bayesian classifier is an algorithm for classifying multiclass datasets. ‚ùñThis is based on the Bayes‚Äô theorem in probability theory. ‚ùñThe classifier is also known as ‚ÄúNaive Bayes Algorithm‚Äù. ‚ùñConditional Probability-The probability of the occurrence of an event A given that an event B has already occurred is called the conditional probability of A given B and is denoted by P(A/B). ‚ùñBayes‚Äô theorem-Let A and B any two events in a random experiment. If P(A) ‚â† 0, then Naive Bayes Algorithm- Assumption and Background idea: ‚ùñAll the features are independent and are unrelated to each other. ‚ùñThe data has class-conditional independence ‚ùñSuppose we have a training data set consisting of N examples having n features. ‚ùñIn this case N=10 and n=4 Naive Bayes Algorithm- Assumption and Background idea: ‚ùñLet the features be named as (F1,‚Ä¶‚Ä¶‚Ä¶,Fn). ‚ùñA feature vector is of the form (f1, f2, ‚Ä¶‚Ä¶‚Ä¶‚Ä¶.,fn). ‚ùñAssociated with each example, there is a certain class label. ‚ùñLet the set of class labels be {c1, c2, ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶,cp}. (Here p=3) ‚ùñSuppose we are given a test instance having the feature vector ‚ùñWe are required to determine the most appropriate class label that should be assigned to the test instance X ‚ùñFor this purpose we compute the following conditional probabilities Naive Bayes Algorithm- Assumption and Background idea: ‚ùñchoose the maximum among them. ‚ùñLet the maximum probability be ‚ùñThen, we choose as the most appropriate class label for the training instance having X as the feature vector. ‚ùñThe conditional probability is computed using Baye‚Äôs Theorem, ‚ùñDue to class conditional independence Naive Bayes Algorithm- Assumption and Background idea: ‚ùñDue to class conditional independence ‚ùñSubstituting in Naive Bayes Algorithm- Assumption and Background idea: ‚ùñDue to class conditional independence ‚ùñSubstituting in Naive Bayes Algorithm- Assumption and Background idea: Naive Bayes Algorithm- Assumption and Background idea: Naive Bayes Algorithm Naive Bayes Algorithm Example For simplicity, each unit is classified as ‚ÄúAnimal‚Äù, ‚ÄúBird‚Äù or ‚ÄúFish‚Äù. Use naive Bayes algorithm to classify a particular species if its features are (Slow, Rarely,No) Example Example Example Example Example Example Example Example Example"
  ],
  "2833fdf5-1ac8-452f-8528-91bd36842170.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Basics of parameter estimation ‚ó¶Maximum Likelihood Estimation (MLE) ‚ó¶Maximum A Posteriori Estimation (MAP) MLE ‚ùñIf the sample are truly random, then they are good approximations of true probabilities, having an underlying probability distribution. These forms are defined by probability functions or probability density functions. ‚ùñMaximum likelihood estimation (MLE) is particular method to estimate the parameters of a probability distribution. ‚ùñMLE attempts to find the parameter values that maximize the likelihood function, given the observations. The resulting estimate is called a maximum likelihood estimate, which is also abbreviated as MLE. MLE ‚ùñSuppose we have a random sample X = {x1, ‚Ä¶‚Ä¶, xn} taken from a probability distribution having the probability mass function or probability density function p(x/Œ∏) where x denotes a value of the random variable and Œ∏ denotes the set of parameters that appear in the function. ‚ùñIn maximum likelihood estimation, we find the value of Œ∏ that makes the value of the likelihood function maximum. MLE ‚ùñFor computation convenience, we define the log likelihood function as the logarithm of the likelihood function: ‚ùñA value of Œ∏ that maximizes L(Œ∏) will also maximise l(Œ∏) and vice-versa. ‚ùñHence, in maximum likelihood estimation, we find Œ∏ that maximizes the log likelihood function. MLE-Example MAP ‚ùñ is the posteriori probability and P(B) is called the prior probability. ‚ùñIn MAP, we try to find the value of B, such a way to maximize the posterior probability"
  ],
  "7eefe247-0fb0-4cd4-8194-850a56d709af.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET Syllabus-Module I ‚ùëMachine learning paradigms-supervised, semi-supervised,unsupervised, reinforcement learning. ‚ùëIntroduction to artificial neurons, Perceptron ‚Äì perceptron algorithm, Neural Network - Multilayer feed forward network, Activation functions (Sigmoid, ReLU, Tanh), Backpropagation algorithm. ‚ùëBasics of parameter estimation - maximum likelihood estimation (MLE) and maximum a posteriori Estimation (MAP). Syllabus-Module II ‚ùëRegression - Linear regression with one variable, Linear regression with multiple variables, Logistic regression, gradient descent algorithm and matrix method, basic idea of overfitting in regression. ‚ùëClassification- Introduction to Bayes decision theory, discriminant functions and decision surfaces. Syllabus-Module III ‚ùëEnsemble methods: boosting, bagging. Decision trees algorithm ID3, random forest, Bootstrapping. ‚ùëSVM - Introduction, Maximum Margin Classification, soft margin SVM classifier, non-linear SVM, Kernels for learning non-linear functions - polynomial kernel, Radial Basis Function (RBF). Syllabus-Module IV ‚ùëClustering - Similarity measures, Hierarchical Agglomerative Clustering, K-means partitional clustering, Expectation maximization (EM) for soft clustering. ‚ùëDimensionality reduction: principal component analysis, Fischer‚Äôs discriminant analysis. Syllabus-Module V ‚ùëClassification Performance measures Confusion matrix,Precision, Recall, Accuracy, F-Measure, Receiver Operating Characteristic Curve (ROC) and Area Under Curve (AUC) and their significance. ‚ùëCross Validation, Bias-Variance decomposition. ‚ùëCase Study: Developing an effective machine learning model for a real world classification or a regression problem. Course Outcomes Level CO1 Explain the basics of machine learning and neural networks. Understan d CO2 Discuss regression and classification theory in machine learning. Understan d CO3 Apply mathematical methods in machine learning models. Apply CO4 Explain supervised and unsupervised learning algorithms and non-metric methods. Understan d CO5 Summarize ensemble methods, dimensionality reduction, evaluation, model selection. Apply Text Books 1. Bishop, C. M. ‚ÄòPattern Recognition and Machine Learning‚Äô, Springer, New York,2e, 2016. 2. Ethem Alpaydin ‚ÄòIntroduction to Machine Learning (Adaptive Computation and Machine learning) 3e, MIT Press, 2014. 3. Mitchell.T, ‚ÄòMachine Learning‚Äô McGrawHill, 1983 Assessment Pattern Continuous Assessment : End Semester Examination 40 : 60 Assignments: Assignment 1-based on all tutorials Assignment 2- Simulation What is Machine learning Machine Learning is all about machines learning automatically without being explicitly programmed or learning without any direct human intervention. This process starts with feeding them good quality data and then training the machines by building various machine learning models using the data and different algorithms. Machine learning allows computers to learn and infer from data. What is Machine learning Machine Learning algorithm learns from experience E with respect to some type of task T and performance measure P For example, if a ML algorithm is used to play chess. Then the experience E is playing many games of chess, the task T is playing chess with many players, and the performance measure P is the probability that the algorithm will win in the game of chess. Supervised learning Types of machine learning ‚ùëSupervised Learning ‚ùëUnsupervised Learning ‚ùëSemi Supervised Machine Learning- ‚ó¶This is a combination of Supervised and Unsupervised Machine Learning ‚ó¶Uses a little amount of labeled data like Supervised Machine Learning and a larger amount of unlabeled data like Unsupervised Machine Learning to train the algorithms. Types of machine",
    "learning Reinforced Machine Learning- ‚ó¶learn optimal actions through trial and error. ‚ó¶algorithm decides the next action by learning behaviors that are based on its current state and that will maximize the reward in the future ‚ó¶This is done using reward feedback Data in machine learning DATA: It can be any unprocessed fact, value, text, sound, or picture that is not being interpreted and analyzed. INFORMATION: Data that has been interpreted and manipulated and has now some meaningful inference for the users. KNOWLEDGE: Combination of inferred information, experiences, learning, and insights. How we split data in Machine Learning Training Data-: The part of data we use to train our model. This is the data that your model actually sees(both input and output) and learns from. Validation Data-The part of data that is used to do a frequent evaluation of the model, fit on the training dataset along with improving involved hyperparameters (initially set parameters before the model begins learning). Testing Data-The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
  ],
  "619ddea7-7177-4d10-978c-736ece9ddd3f.pdf": [
    "Machine Learning for Data Processing (23ECL2MD) DR.SWAPNA P S ASSOCIATE PROFESSOR MBCET BIOLOGICAL NEURAL NETWORK BIOLOGICAL NEURAL NETWORK ‚Ä¢ 10^11 neurons in total ‚Ä¢ each connected to 10^4 neurons on average ‚Ä¢ Multiple signals arrive at the dendrites and are then integrated into the cell body, and, if the accumulated signal exceeds a certain threshold, an output signal is generated that will be passed on by the axon. Artificial Neural Network (ANN) The study of artificial neural networks (ANNs) has been inspired in part by the observation that biological learning systems are built of very complex webs of interconnected neurons. In rough analogy, artificial neural networks are built out of a densely interconnected set of simple units, where each unit takes a number of real-valued inputs (possibly the outputs of other units) and produces a single real-valued output (which may become the input to many other units). Artificial Neural Network (ANN) While ANNs are loosely motivated by biological neural systems, there are many complexities to biological neural systems that are not modelled by ANN. Many features of the ANNs are known to be inconsistent with biological systems. For example, in ANNs, individual units output a single constant value, whereas biological neurons output a complex time series of spikes. Artificial Neuron The artificial neuron has the following characteristics: ‚Äì A neuron is a mathematical function modelled on the working of biological neurons ‚Äì It is an elementary unit in an artificial neural network ‚Äì One or more inputs are separately weighted ‚Äì Inputs are summed and passed through a nonlinear function to produce output ‚Äì Each connection link carries information about the input signal ‚Äì Every neuron is connected to another neuron via connection link Perceptrons Perceptrons A perceptron takes a vector of real-valued inputs, calculates a linear combination of these inputs, then outputs a 1 if the result is greater than some threshold and -1 otherwise. A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features in the input data. Perceptron Perceptron was introduced by Frank Rosenblatt in 1957. A Perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurons to learn and processes elements in the training set one at a time. There are two types of Perceptrons: Single layer and Multilayer. ‚ó¶Single layer Perceptrons can learn only linearly separable patterns. ‚ó¶Multilayer Perceptrons or feedforward neural networks with two or more layers have the greater processing power. ‚Ä¢ ‚ó¶The Perceptron algorithm learns the weights for the input signals in order to draw a linear decision boundary. ‚Ä¢ This enables you to distinguish between the two linearly separable classes +1 and -1. Perceptrons Weight parameter represents the strength of the connection between units. Activation Function: These are the final and important components that help to determine whether the neuron will fire or not. Activation Function can be considered primarily as a step function. Sign function Step function Sigmoid function Activation Functions Perceptrons Perceptrons Perceptrons CS 270 - PERCEPTRON 15",
    "Perceptron Node ‚Äì Threshold Logic Unit q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 z x1 xn x2 w1 w2 wn qùúÉ CS 270 - PERCEPTRON 16 Perceptron Learning Algorithm x1 x2 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 .4 -.2 .1 x1 x2 t 0 1 .1 .3 .4 .8 CS 270 - PERCEPTRON 17 First Training Instance 0.8 0.3 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 0.4 -0.2 .1 net = 0.8*0.4 + 0.3*-0.2 = 0.26 =1 x1 x2 t 0 1 0.1 0.3 0.4 0.8 CS 270 - PERCEPTRON 18 Second Training Instance .4 .1 z q q < = = = i n i i i n i i w x z w x 1 1 if 0 if 1 .4 -.2 .1 x1 x2 t 0 1 .1 .3 .4 .8 net = .4*.4 + .1*-.2 = .14 =1 ÔÅÑwi = (t - z) ÔÄ™ c ÔÄ™ xi Perceptron Training Rule precise learning problem is to determine a weight vector that causes the perceptron to produce the correct f 1 output for each of the given training examples. y to learn an acceptable weight vector is to begin with random weights, then iteratively apply the perceptron to each training example, modifying the perceptron weights whenever it misclassifies an example. This process is repeated, iterating through the training examples as many times as needed until the perceptron classifies all training examples correctly. The role of the learning rate is to moderate the degree to which weights are changed at each step. It is usually set to some small value (e.g., 0.1) CS 270 - PERCEPTRON 20 Perceptron Rule Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: ÔÅÑwi = c (t ‚Äì z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) ÔÅÑW 0 0 1 1 0 0 0 0 0 CS 270 - PERCEPTRON 21 Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: ÔÅÑwi = c(t ‚Äì z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) ÔÅÑW 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0",
    "CS 270 - PERCEPTRON 22 Example Assume a 3 input perceptron plus bias (it outputs 1 if net > 0, else 0) Assume a learning rate c of 1 and initial weights all 0: ÔÅÑwi = c (t ‚Äì z) xi Training set 0 0 1 -> 0 1 1 1 -> 1 1 0 1 -> 1 0 1 1 -> 0 Pattern Target (t) Weight Vector (wi) Net Output (z) ÔÅÑW 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶ Perceptrons-Hyperplanes We can view the perceptron as representing a hyperplane decision surface in the n- dimensional space of instances (i.e., points). The perceptron outputs a 1 for instances lying on one side of the hyperplane and outputs a -1 for instances lying on the other side Perceptrons Perceptrons"
  ]
}